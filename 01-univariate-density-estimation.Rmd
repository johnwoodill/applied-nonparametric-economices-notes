---
title: 'Chapter 2: Univariate density estimation'
output: html_document
---

# Univariate density estimation

## Introduction

A simple nonparametric density estimation is the histogram.  Assuming a pre-specified parametric density can show information that may be hidden otherwise.  However, the shape of the histogram is determined by the number bins assigned (binwidth) so careful consideration of the number of bins should be considered.  Another issue to be aware of is histograms are discontinuous in nature and important information may be omited (e.g. gradients).  The kernel density estimator provides a natural smoothing estimator that is continuous.

## Smoothing preliminaries

Kernel smoothing is an important method for estimating the density function; however, with insufficient smoothing the data may provide spurios information, or when over smoothing occurs, the information may be masked.  Thus, it is important to ensure kernel smoothing is done to extract the appropriate level of data.

Types of kernel densities:

```{r pressure, echo=FALSE}
par(mfrow=c(2,2))
x <- seq(-4,4,length=200)
y <- dnorm(x,mean=0, sd=1)
plot(x,y, type = "l", lwd = 2, xlim = c(-3.5,3.5), main = "Normal density")

x <- seq(0,6,length=200)
y <- dlnorm(x, mean=0, sd=1)
plot(x,y, type = "l", lwd = 2, xlim = c(0,6), main = "Log-normal density")

x <- seq(-4,4,length=200)
y <- dt(x, 5)
plot(x,y, type = "l", lwd = 2, xlim = c(-3.5,3.5), main = "Student's t")

x <- seq(-4,4,length=200)
y <- dnorm(x,mean=0, sd=1)
y <- density(y)
plot(x=y$x*10, y=y$y/10, type = "l", main = "Mixture of normals")
#plot(x,y, type = "l", lwd = 2, xlim = c(-3.5,3.5))
```

The smoothing parameter is an important control for uncovering features of the distribution.  The standard smooth parameter (`bw.nrd0`) defauls to 0.9 times the minimum of the standard deviation and the interquartile range divided by 1.34 times the sample size to the negative one-fifth power (See ??bandwidth and Silverman, 1986 page 48).  This rule-of-thumb automatically applies to the kernel density estimator, but can be changed by selecting a different bandwidth (e.g. smoothing based on one-tenth the variation).  By changing the smoothing parameter the shape of the distribution will change.  

```{r pressure1, echo=FALSE}
set.seed(201010)
x <- rnorm(1000, 10, 2)
par(mfrow = c(2,2))
plot(density(x))  #A bit bumpy
plot(density(x,adjust = 10)) #Very smmoth
plot(density(x,adjust = .1)) #crazy bumpy
```


The figure above adjusts the bandwidth and shows the changes in the kernel density estimator.  It is important to note the degree of smoothness is no more arbitraty than selections in parametric analysis.  The authors argue "the choice of parametric model is typically as ad hoc as picking the smoothing parameter, sometimes seemingly at random in applied economics."  The arbitrary selection in parametric analysis is just as selective to key insights as in nonparametric analysis.  Researchers should be aware of these choices when understanding important economic relationships from model selection.

## Estimation

### A crude estimator (histogram)

Suppose $S(x)=[a,b]$ is a subset of the domain not mapped to zero (support) where the binwidth is defined as $h=(b-a)/J$ where $J$ are number of boxes in histogram.  The interval is defined as $(a-(j-1)h,a+jh]$ for $j=1,2,...,J$.  Therefore, the number of observations $n_j$ in bin $j$ is defined as:

$$n_j=\sum_{i=1}^n \mathbf{1} \{ \underbrace{a+(j-1)h < x_i \leq a+jh\}}_{\text{A}}$$


where $1{A}$ is 1 if A is true and zero otherwise.  The Expected proportion of observations in jth interval can then be defined as:

$$\frac{E[n_j]}{n} = Pr(a+(j-1)h < x \leq a+jh)= \int_{a+(j-1)h}^{a+jh} f(x)dx$$

where $f(x)$ is the density of x.  Assuming $J$ is large $h$ becomes small.  Approximation can be assumed to be a constrant of $f(x)$ where $f(x) \approx f(c)$:

$$\int_{b}^{a} cdx = cx \bigg|_{a}^{b} = c(b-a)$$
Expectation of the value of density function in interval $(a+(j-1)h, a+jh])$:

$$\frac{E[n_j]}{n} = \int_{a+(j-1)h}^{a+jh} f(x)dx \approx f(c) \underbrace{(a+jh-(a+(j-1)h)}_{\text{a+jh-a-jh+h=h} })=hf(c)$$
$$\frac{E[n_j]}{n}=hf(c)$$
Dividing by $h$, the expected density function is defined as:

$$\hat{f}(c)=\frac{n_j}{hn} \hspace{5mm} \text{where} \hspace{5mm} c \in (a+(j-1), a+jh]$$ 

--------------------------

**Bias**

Unless $c$ is exact point of interest (unlikely) the estimator is biased, discontinuous, and indifferentiable.  Bias for estimator $\hat{f}(c)$ is defined as:

$$\text{Bias}[\hat{f}(c)]=E[\hat{f}(c)]-f(c) = E[\frac{n_j}{hn}]-f(c)$$ 

$$\text{Bias}[\hat{f}(c)]=\frac{1}{h} \int_{a+(j-1)h}^{a+jh} f(x)dx - f(c)$$

The mean value theorem says a point can exist between two values in an integral, such as $f'(c) = \frac{f(b)-f(a)}{b-a}$, therefore allowing the bias to reduced to the form:

$$\text{Bias}[\hat{f}(c)]=f(\tilde{a})(a+jh - (a+(j-1)h))$$

$$\text{Bias}[\hat{f}(c)]=hf(\tilde{a}) \hspace{5mm} \text{for} \hspace{5mm} \tilde{a} \in ((j-1)h,a+jh]$$

If $c \equiv \tilde{a}$ then the estimator is unbiased:

$$\text{Bias}[\hat{f}(c)]=\frac{1}{h}hf(\tilde{a}) - f(c)$$

$$\text{Bias}[\hat{f}(c)]=f(\tilde{a}) - f(c) = 0$$
It is important to note that bias does not depend on the sample size and is directly controlled by the binwidth.

--------------------------

**Variance (precision)**

The variance of the estimator is defined as:

$$Var[\hat{f}(c)] = Var(\frac{n_j}{nh}) = \frac{Var(n_j)}{(nh)^2}$$

Know that $n_j$ is equal to the number of observations in interval and the probability of falling in the interval is defined as $p=f(\hat{a})h$ for $\tilde{a} \in (a+(j-1)h, a+jh)$ where a binomial distribution of $z \sim Bi(n,p) = np(1-p)$  Therefore, the variance can be expressed as:

$$Var[\hat{f}(c)] = \frac{np(1-p)}{(nh)^2}= \frac{f(\tilde{a})h(1-f(\tilde{a})h)}{nh^2} =  \frac{f(\tilde{a})h(1-f(\tilde{a})h)}{nh} $$

As $h \rightarrow 0$ the variance, $Var[\hat{f}(c)]$ increases.  Therefore, in the variance of the estimator, the sample size influences the size of the variance -- as the sample size increases the variance decreases.  

The design of the estimator ($f(\tilde{a})$) changes the bias and variance so different designs may provide better bias and variance of the estimator.  The conflict between the bias and variance is a fundamental point in nonparametric estimation.

* If $nh \rightarrow \infty$ and $h \rightarrow 0$ the bias and variance decreases, thus converging to mean square error.
  
* More info on sample allows for the partitioning of $S(x)$, thus decreasing the bandwidtch and lowering the bias
  
* However, $h$ cannot be lowered to offset gains in information, thus $\frac{h}{n} \rightarrow 0$.

------------------------------

**R Example**: A general histogram density estimator

```{r histogram_estimator}
# Sample data
set.seed(1234)
x <- rnorm(100)

# Function to plot histogram with bins
histogram <- function(x, bins = 5){
  S <- range(x)
  a <- range(x)[1]
  b <- range(x)[2]
  J <- bins
  h <- (b - a)/J
  n <- integer(J)
  
  for (j in 1:J){
    for (i in 1:length(x)){
      if(x[i] > a + (j-1)*h && (x[i] <= a + j*h)) {n[j] <- n[j] + 1L}
    }
  }
  
  # Manually add +1 to n[1] because lowest value is not included
  n[1] <- n[1] + 1
  barplot(n)
  return(n)
}

histogram(x, 10)
```

An expected density function where $\hat{f}(c) = \frac{n_j}{nh}$ can be plotted in R as:

```{r hist_dens_estimator}
# Sample data
set.seed(1234)
x <- rnorm(100)

# Function to plot histogram with bins
histogram <- function(x, bins = 5){
  S <- range(x)
  a <- range(x)[1]
  b <- range(x)[2]
  J <- bins
  h <- (b - a)/J
  n <- integer(J)
  
  for (j in 1:J){
    for (i in 1:length(x)){
      if(x[i] > a + (j-1)*h && (x[i] <= a + j*h)) {n[j] <- n[j] + 1L}
    }
  }
  
  # Manually add +1 to n[1] because lowest value is not included
  n[1] <- n[1] + 1
  return(n)
}

# New vector 
expden <- integer(10)

# Expected density function f(c) = n_j/h*n
density_histogram <- function(x, bins = 5){
    S <- range(x)
    a <- range(x)[1]
    b <- range(x)[2]  
    J <- bins
    h <- (b - a)/J
    binlength <- histogram(x, bins)
    
    for (i in 1:length(binlength)){  
      expden[i] <- binlength[i] / (h*length(x))
    }
     return(expden)
}
par(mfrow=c(3,2))
# With 10 bins
bins <- 10
dat <- data.frame(x = seq(from = min(x), to = max(x), length.out = bins), 
                  y = density_histogram(x, bins))
barplot(histogram(x, bins), main = "Histogram with 10 bins")
plot(dat, type = "l", main = "Density of Histogram with 10 bins")

# With 20 bins
bins <- 20
dat <- data.frame(x = seq(from = min(x), to = max(x), length.out = bins), 
                  y = density_histogram(x, bins))
barplot(histogram(x, bins), main = "Histogram with 20 bins")
plot(dat, type = "l", main = "Density of Histogram with 20 bins")

# With 100 bins
bins <- 100
dat <- data.frame(x = seq(from = min(x), to = max(x), length.out = bins), 
                  y = density_histogram(x, bins))
barplot(histogram(x, bins), main = "Histogram with 100 bins")
plot(dat, type = "l", main = "Density of Histogram with 100 bins")
```

### Naive estimator

With a crude estimator, bias varies considerable.  To overcome this, boxes at each point can be centered on each obs as:

$$n_x=\sum_{i=1}^{n} \mathbf{1} \{ x-h < x_i \leq x+h \}$$

therefore,

$$\hat{f}(x) = \frac{n_x}{2nh} = \frac{1}{2nh} \sum_{i=1}^{n} \mathbf{1} \{ x-h < x_i \leq x+h \}$$

Conveniently, the density estimator can be defined as:

$$\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^{n} (1/2) \mathbf{1} \{ | \frac{x_i - x}{h} | \leq 1 \}$$

The derivative of expected cumulative density function (ECDF) is an equivalent density estimator:

$$\hat{F}(x) = \frac{1}{n} \sum_{i=1}^{n} \mathbf{1} \{ x_i \leq x \}$$

where $f(x) = \frac{d}{dx}F(x)$

The numerical derivative is exactly the same as the naive estimator.

Now, assume the kernel function, $k(\psi)$ is defined as:

$$k(\psi)= (1/2) \mathbf{1} \{| \frac{x_i - x}{h} | \leq 1 \} $$

where $k(\psi)$ is equal to one if $|\psi| \leq 1$

Uniform kernel is defined as the variety of weighting function to construct nonparametric estimates:

* Discontinuous at -1 and 1

* Derivative of zero everywhere by at -1 and 1

* Therefore, the density estimator will not be smooth

For a function to be a probability density it must be non-negative and integrate to 1.  A check on this shows:

$$k(\psi) \geq 0 \hspace{3mm} \forall \hspace{3mm} \psi$$


$$\int_{S(x)} \hat{f}(x)dx = 1$$

-------------------------------------------

### Kernel estimator

A generic definition for a kernel density:

$$\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^{n} K(\frac{x_i - x}{h})$$

Where $h$ is the bandwidth providing smoothness of the estimator.  It is important to select kernel functions that are smoother than uniform kern by selection of $k(\psi)$ and $h$ for correct selection fo estimator.

Let,

$$k_j(k) = \int_{- \infty}^{\infty} \psi^{j}k(\psi)d\psi  $$
If $k_0(k)=1, k_1(k)=0,$ and $k_2(k) < \infty$ is kernal of second order, then imposing integration to unity ($k_0(k)=1$) ensures a PDF.

* $k_1(k)=0$ allows for symmetritry about zero ($k(- \psi) = k(\psi))$)

* $k_2(k) < \infty $ allows for finite variance and is necessary for bias of variance estimator

For symmetric kernels $k_l(k)=0$ for $l=2j+1$ (all odd)

**Figure of most common kernels**

![](/run/media/john/1TB/Projects/applied-nonparametric-economices-notes/Figures/common_kernel_functions.png)

![](/run/media/john/1TB/Projects/applied-nonparametric-economices-notes/Figures/common_kernel_functions2.png)

All kernels in figure stem from general polynomical family where,

$$k_s(\psi)=\frac{(2s+1)!!}{2^{s+1}s!}(1-\psi^{2} 1 \{ | \psi | \leq 1 \}$$

where $(2s+1)!!=(2s+1)(2s-1) \cdot \cdot \cdot 5 \cdot 3 \cdot 1$ and $s$ is biweight kernel ($s=2$) "quartic kernel."  As s increases the kernel gains additional derivatives.

Reweighting kernel to Gaussien kernel gives a PDF of standard nromal with derivaties of all orders.  Reweighting to a scaled kernel ($b^-1 k * (\frac{\psi}{b}$ with bandwidth ($h*=\frac{h}{b}$) produces an identical estimate of $k(\psi)$ and $h$

$$\hat{f}(x) = \frac{1}{nh} \sum_{i=1}^{n}k (\frac{x_i-x}{h})$$
$$\hat{f}(x) = \frac{1}{nh/b} \sum_{i=1}^{n} b^{-1} k (\frac{x_i-x}{bh/b})$$

$$\hat{f}(x) = \frac{1}{nh*} \sum_{i=1}^{n}k* (\frac{x_i-x}{h*})$$

Selecting the correct scale for the data ensures appropriate estimation.

-------------------------------------------

**Bias and Variance**

With basic properties established we can now analyze the bias and variance.  To provide bandwidth and kernel selection, the bias of kernel density estimation is:

$$Bias[\hat{f}(x)]=E[\hat{f}(x)] - f(x) \approx \frac{h^2}{2} f^{(2)}(x)k_2(k)$$

Where $f^{(2)}$ is the second order derivative.  In nonparametric analysis the bias reveals:

* Bias is independent of sample size

* Bias depends on design of $f^{(2)}.  Certain portions of density may have lower bias.  The steeper the density, the larger the bias (larger $f^{(2)})$$

* Bias depends on kernel used in $k_2(k)$

* Bias depends directly on bandwidth.  As the bandwidth decreases, estimation bias decreases; however, decreasing bandwidth also icnreases variance.

Variance of density estimator can be derived as (See technical appendix 2.4 for detailed derivation):

$$Var[\hat{f}(x)] \approx \frac{1}{nh} f(x) R(k)$$

where $R(k) = \int g(\psi)^{2} d\psi$ and is thought of as the "difficult" of kernel function.

The variance depends on sample size, bandwidth, design, and kernel employed.  Reducing the bandwidth increases the variance, but a var decrease with increasing sample size.  Combingin bias and variance into an asymptotic mean square error (AMSE) of density estimator:

$$AMSE[\hat{f}(x)] = \text{Bias}[\hat{f}(x)]^2 + \text{Var}[\hat{f}(x)] $$

$$AMSE[\hat{f}(x)] = \frac{k_2^2(k)}{4} f^{(2)}(x)^2h^4+(nh)^{-1} f(x)R(k)$$

and shows the bias-variance trade off where the bias can be reduced at the expense of variance.  As $n \rightarrow \infty, h \rightarrow 0$, but bandwidth decreases slow enough so $nh \rightarrow \infty$.  If $h$ decreases too rapidly then gains in bias reduction are offset by increases in variance.  This is the fundamental relationship between bias and variance in nonparametric estimation.

## Kernel selection

Which kernel result has the lowest AMISE?  A calculus of variation can minimize the AMSE such that:

$$min_k(\Psi) : AMISE[\hat{f}(x)] \hspace{5mm} \text{s.t.} \hspace{2mm} K_{0}(k) = 1; K_1 (K) = 0; K_2 (k) < \infty$$

See Epanechnikov (1969), Serfling (1980) and Miller (1984)

## Kernel efficiency

If both density estimators contain optimal bandwidths, relative efficiency can be determined by comparing the ratio of AMISE for each.

$$Eff (K_Q) = \frac{R(K_Q) K_2 (K_Q)^{1/2}}{R(K_1)}$$

If $K_2(K) = 1$, then $Eff = \frac{R(K_Q)}{R(K1)}$

Note: Epanechnikov kernel results in the loest AMISE it does not impy it is the best primaryly because

* Epanechnikov contains only one derivative where may exceed multiple derivaties

* Kernel optimallty \doesnotequal optimal AMISE kernel

When selecting density estimator it is important to know the pentaly of different estimators and the need for additional obs to overcome this pentalty.

**See table 2.2**

## Bandwidth Selection

Selection of bandwidth is anoter important mechanism for kernel estimation.

#### Optimal selection

The optimal bandwidth w.r.t minimuzing AMSE is :

$$h_{opt} = \Bigg[\frac{f(x)R(k)}{k_2^2(k) f^2 (x)^2} \Bigg]^{1/5} n^{-1/5}$$

Where $n^{-1/5}$ is the rate at which information is used by the bandwidth to decrease the bias.

Appropriate scale factors are shown in table 2.3:

### Data driven methods

When the underlying density is not normal, it is desirable to have access to methods tailored to the underlying design.

**Plug-in-method**

The difficulty with obtaining optimal bandwidth selectors comes from unknown roughness of density $R(f^{(2)})$.  Repeated integration by parts of $\hat{R}(f^{(r)}) = $

$$\hat{R}(f^{(r)}) = n^{-2} \check{h}^{(-1+r)} \sum_{i=1}^{n} \sum_{j=1}^{n} k^{2r} \frac{x_{i}-x_{j}}{\check{h}}$$

The natural estimtor depends on choice of kernel and bandwidth with a few caveats.  First, the optimal bandwidth is: 

$$\check{h}= \Bigg[\frac{(-1)^{r/2}2k^{(2r)}(0)}{k_{2}(k)R(f^{(2r+2)})n} \Bigg]^{1/(2r+2+1)}$$

In order to estimate $R(\hat{(2)}$ we need to know the bandwidth for $R(\hat{f}^{4})$

THe direct plug-in approach is an $l$-step algorithm for estimating $R(f^{2l+2})$ which is then "plugged in" for estimate $R(f^{2(l-1)+2)})$.  This is repeated until $R(f^{(2)})$ is obtained.  The Silverman approach uses a zero-step plug-in algorithm.

**Least-sequares cross validation (LSCV)**

The most popular approach is LSCV with a goal of minimizing the difference between the estimator density and the density itself.  The integrated equare error is defined as:

$$\text{ISE}(\hat{f},f) = \int \hat{f}(x) - f(x)^{2}dx$$

therefore, minimizing with respect to $h$

$$\text{ISE}^*(\hat{f},f)= \int \hat{f}(x)^{2}dx-2 - 2 \int \hat{f}(x) f(x)dx$$

Because $f(x)$ is unknown a common approach is to leave one out so bandwidth selector does not exist for zero.  A simplification of estimating LSCV(h):

$$LSCV(h) = \frac{1}{n^2 h} \sum_{i=1}^{n} \sum_{j=1}^{n} \check{k} (\frac{x_{i - x_j}}{h})$$

This formulation condeses terms to allow an easier computation than leave-one-out

Two ways for finding optimal $h$ is to:

* Grid search, but bounds on grids may be a problem

* Define rate ahead of time and optimize LSCV to evaluate the scale factor.  This may cause issues w/ over or undersmoothing the estimate

**Likelihood Cross-validation**

An easier estimation than LSCV is using likelihood-cross valudation (LCV).  The central idea is to select a bandwidth that maximizes LCV criterion to the true density (Kullback-Leisler Distance)

$$KL(f, \hat{f}) = \int f(x) log{f(x)/\hat{f}(x)}dx$$
LCV selects a bandwidth where additional observse the density estimator produces the highest likelihood.  Using leave-one-out, the average over data is estimated as:

$$LCV(h) = n^{-1} \sum_{i=1}^{n} log\hat{f}_{i}(x_{i})$$
One issue with LCV is it can produce inconsistent estimates as $h \rightarrow \infty$ due to distances between data points and sample sizes (e.g. fat tailed distributions widen, thus increasing inconsistency in data).  Kernel selection is important when using (LCV).

## Plug-in or cross-validation?

No clean consensus on the appropriate method for bandwidth selection.  One easy way of comparing bandwidth selections is to understand how uncertainty is manifested.  Understanding this uncertainty is key to bandwidth selection.

If smoother estimates are desired, then AMISE may need be desireable because it focuses more son bias and variance.  Care in selecting smoothing estimates should be taken using plug-in method due to possibly oversmoothing.  It should be acknolwedged it is up to the researcher to understand and select the appropriate bandwidth selector.

## Density Derivaties

Interests in derivative may be of interest instead of the dentsity (e.g. methods requiring estimation of second derivative, Silverman)

An estimateor of $r^{th}$ order derivative is:

$$\hat{f}^{r}(x) = \frac{d^r}{dx^r} \hat{f}(x) = \frac{1}{n h^{r+1}} \sum_{i=1} k^{r} (\frac{x_{i}-x}{h})$$
If sample size is small, and curves are not differentiable then estimates might be jumpy (e.g. EPD Neichnikov kernal is not differentiable).

## Bias and Variance

$$Bias[\hat{f}^{r}(x)] \approx \frac{h^2}{2} f^{r+2} (x) k_2(k)$$
The bias is of the same order and identical up to scale with the bias of our initial density estimator, with the only difference being $f^{(2)}(x)$ derives bias of $\hat{f}(x)$.  The variance, however, is not so easily estimated.

$$Var[\hat{f}^{(r)}](x) \approx \frac{f(x) R(k^{(r)}}{nh^{1+2r}}$$
The magnitude of AMSE and AMISE is therefore larger.  Estimates of density with derivaties is not as precise.  The derivatives magnify as the derivative order is integrated.

### Bandwidth selection


A derivated equivalent under unknown density:

$$AMSE \approx \frac{h^4}{4} f^{(r+2)}(x)^{r} k_2^2(k) + \frac{f(x)R(k^{(r)})}{nh^{1+2r}}$$

However, the bandwidth convergence decays slower because of variance issues discussed above.  See page 49 for AMISE discussion adn optimal bandwidth selection which is omitted here.

### Relative Efficiency

Estimating between two kernels is down much the same way as without the derivative

$$Eff(k_Q, r)= \frac{R(k_Q^{(r)})}{R(k_{r+1}^{(r)})} \Bigg(\frac{k_2^2(K_Q)}{k_2^2(K_r+1)})\Bigg)^{(1+2r)/4}$$

Table 2.5 and 2.6 provide kernel efficiency for $r=1,2,3,4$ derivatives

